== Book Notes from Algorithm Design Manual 2nd Edition
I'll be reading this book and hopefully posting notes and code snippets for each
category of algorithms

== Chapter 1: Introduction to Algorithm Design
[source,C]
----
// insertion sort
void insertion_sort(int *items, int n) {
    int i;
    int j;

    for (i = 1; i < n; ++i) {
        j = i;
        while ((j != 0) && (items[j] < items[j - 1])) {
            swap(&items[j], &items[j - 1]);
            --j;
        }
    }
}
----

=== Algorithm vs Heuristics
Algorithm always produces a correct result, whereas heuristics do a good job
without providing any guarantee.

=== Proof by induction
Often mathematical induction is the method of choice to prove that an algorithm
is valid.

You prove a formula for some basis case like 1 or 2, then assume it is true all
the way till `n - 1` before proving it was true for general _n_ using the
assumption.

Recursion: program tests whether the input was some basis case 1 or 2. If not,
solve the bigger case by breaking it into pieces and calling the subprogram
itself to solve these pieces.

Recursion is mathematical indcution.

Proving the correctness of insertion sort...

- Basis case consists of a single elemnt, and a single element array is
  completely sorted
- In general, we can assume that the first `n - 1` elements of array A are
  completely sorted after `n - 1` iterations of insertion sort
- To insert one last element x to A, we find where it goes (between biggest
  element less than or equal to x and the smallest element greater than x). Done
  by moving all the greater elements back by one position, creating room for x
  in the desired location.

==== Inductive Proof subtle errors
- Boundary errors:: greater care is needed to properly deal with the special
  cases of inserting the minimum or maximum elements
- Cavallier extension claims:: more common class of error. e.g scheduling
  problem: adding a new segment might now contain none of the segments of any
  particular optimal solution prior to insertion

=== Modeling the Problem
Modeling is formulating your application in terms of precisely described,
well-understood problems.

Trees:: hierarchical relationships between items
Graphs:: relationships between arbitrary pair of objects. e.g Networks

Polygons:: regions in some geometric spaces.


== Chapter 2: Algorithm Analysis
=== RAM Model of Computation
It is a hyptothetical computer called the Random Access Machine.

* Each simple operation (+, -, *, if, call) takes exactly one time step
* Loops are not considered simple oeprations.
* Each memory access takes exactly one time step

We measure runtime by counting up the number of steps an algorithm takes on a
given problem instance.

=== Big Oh Notation
It ignores the difference between multiplicative constants.

----
O(n) -> upper bound. (Big Oh)

f(n) = O(g(n))
Condition: f(n) <= c g(n)


Ω(n) -> lower bound

f(n) = Ω(g(n))
Condition: f(n) >= c g(n)

Θ(n) --> upper and lower bound

f(n) = Θ(g(n))
Condition:  c~1~ g(n) <= f(n) <= c~2~ g(n)
----

=== Types of functions
----
Constant:: f(n) = 1

Logarithmic:: f(n) = log n

Linear:: f(n) = n

Superlinear:: f(n) = n lg n

Quadratic:: f(n) = n^2^

Cubic:: f(n) = n^3^

Exponential:: f(n) = c^n^

Factorial:: f(n) = n!

n! > 2^n^ > n^3^ > n^2^ > n lg(n) > n > lg(n) > 1
----

=== Working with Big Oh

O(f(n)) + O(g(n)) -> O(max(f(n), g(n)))
Ω(f(n)) + Ω(g(n)) -> Ω(max(f(n), g(n)))
Θ(f(n)) + Θ(g(n)) -> Θ(max(f(n), g(n)))


So, `n^3^ + n^2^ + n + 1 = O(n^3)`

Intuition: At least half the bulk of `f(n) + g(n)` must come from the larger value
as `n -> ∞`. Thus, dropping the smaller function from consideration reduces the
value by at most a factor of half, which is just a multiplicative constant.

Multiplication is like repeated addition (e.g O(c f(n))) -> O(f(n)))

----
# e.g of this kind of multiplication: nested loops
O(f(n)) * O(g(n)) -> O(f(n) * g(n))
Ω(f(n)) * Ω(g(n)) -> Ω(f(n) * g(n))
Θ(f(n)) * Θ(g(n)) -> Θ(f(n) * g(n))
----

=== See discussion on insertion and selection sort in their source code
[source,C]
----
// selection sort
void selection_sort(int *s, int n) {
    int i;
    int j;
    int min;

    for (i = 0; i < n; ++i) {
        min = i;
        for (j = i + 1; j < n; ++j) {
            /* search for anything smaller than i */
            if (s[j] < s[min]) {
                min = j;
            }
        }
        /* swap the smallest item with i */
        swap(&s[i], &s[min]);
    }
}
/**
 * e.g
 * 64 25 12 22 11
 * 11 25 12 22 64
 * 11 12 25 22 64
 * 11 12 22 25 64
 * 11 12 22 25 64
 */
----
==== Analysis of selection sort
----
S(n) = E^n-1^~i=0~  E^n-1^~j=i+1~ 1  = E^n-1^~i=0~ n - i - 1
S(n) = (n - 1) + (n - 2) + (n - 3) + ... + 2 + 1
S(n) = n(n - 1) / 2
----

A basic rule of thumb in Big Oh analysis is that worst-case running time follows
from multiplying the largest number of times each nested loop can iterate.

e.g insertion sort => it's tricky to analyse the inner loop in the nested loop.
We could just assume that it runs `i` times.

This crude ``round it up'' analysis always does the job for Big Oh running time
bound.

=== String Pattern Matching
Substring pattern Matching

I: text string t and a pattern string p

O: Does t contain the pattern p as substring, and if so, where?

[source,C]
----
/* t == string, p == substring */
int findmatch(char *p, char *t) {
    int i;
    int j;
    int m, n;

    m = strlen(p); /* substring */
    n = strlen(t); /* string */

    for(i = 0; i <= (n-m); i++) {
        j = 0;
        while ((j < m) && (t[i + j] == p[j])) {
            j++;
        }
        if (j == m) return i;
    }
    /* failed */
    return -1;
}
----

`O((n-m)(m+2))`

m+2 => inner loops run at most m times, (+2 comes from j = 0 and if statement
inside outer loop)


`O((n-m)(m+2))` simplifies to O(nm - m^2^). Since O() means upper bound, we can get
rid of the -m^2^.

O(nm)


=== Matrix multiplication
----
for (i = 1; i <= x; ++i) {
    for(j = 1; j <= y; ++j) {
        c[i][j] = 0;
        for(k=1; k <= z; ++k) {
            c[i][j] += A[i][k] * B[k][j];
        }
    }
}
----
O(n^3^)

=== Logarithms and Their Applications
==== Binary Search
O(log n)

A binary tree of height 1 can have up to 2 leaf nodes, while a tree of height 2
can have up to 4 leaves.

To account for n leaves:

n = 2^h^

h = log~2~ n

==== Exponentiation
If we want to do a^n^, we could do so by:

a^n^ = (a^(n/2)^)^2^ <- even

a^n^ = a(a^(lower(n/2)^)^2^ <- odd

We have halved the size of our exponent at the cost of, at most, two
multiplications.

So O(lg n) multiplications needed

----
func power(a, n)
    if (n = 0) return 1
    x = power(a, lower(n/2))
    if (n is even) then return(x^2)
    else return (a * x ^ 2)
----

This illustrates an important principle of divide and conquer. It always pays to
divide a job as evenly as possible.

Logarithms arise whenever things are repeatedly halved or doubled.

== Chapter 3: Data Structures
Changing a data structure in a slow program can work the same way an organ
transplant does in a sick patient.

Data structures can be classified as either contiguous or linked, depending upon
whether they are based on arrays or pointers.

- Contiguous => array, heaps, hash tables

- Linked => composed of distinct chunks of memory bound together by pointers,
and include lists, trees, and graph adjacency lists

Advantage of contiguous:
- Constant-time access given the index
- Space efficiency
- Memory locality

Downsize of arrays is that we cannot adjust their size in the middle of a
program's execution.


Pointers:
[source,C]
----
typedef struct list {
    item_type item;
    struct list *next;
} list;
----

Stack: pust: pop
Queue: enqueue, deque
Dictionary: search(k), insert(x), delete(x)

Additional features of dictionary: max, min -> find item with largest key <--
used in priority queue.

View binary search as a binary tree.

[source,C]
----
void tranverse_tree(tree *t) {
    if (t != NULL) {
        traverse_tree(t->left);
        process_item(t->item);
        traverse_tree(t->right);
    }
}

void insert_tree(tree **t, item_type x, tree *parent) {
    tree *p;
    if (*t == NULL) {
        p = malloc(sizeof(tree));
        p->item = x;
        p->left = p->right = NULL;
        p->parent = parent;
        *t = p;
        return;
    }

    if (x < (*t)->item) insert_tree(&((*t)->left), x, *t);
    else insert_tree(&((*t)->right), x, *t);
}
----

Binary trees are good if the tree is perfectly balanced to implement dictionary
operations (searching, inserting, deleting)

If the user inserts keys in sorted order, this will produce a skinny linear
height tree where only right pointers are used. [1]

Binary trees can have heights ranging from lg n to n.

[1]: To fix [1], what would be better is an insertion/deletion procedure which
adjusts the tree a little after each insertion, keeping it close enough to be
balanced so the maximum height is logarithmic.

==== Priority Queues
e.g schedule jobs according to their importance relative to other jobs.
Scheduling requires sorting them by importance, and then evaluating them in this
sorted order.

Priority queues are data structures that provide more flexibility than simple
sorting, because they allow new elements to enter a system at arbitrary
intervals.

insert, find-minimum, find-maximum, delete-minimum, delete-maximum

Dating is the process of extracting the most desirable person from the data
structure, spending an evening to evaluate them better, and then reinserting
them into the priority queue with a possibly revised score.

We can implement a priority queue using an unsorted array, a sorted array, or a
balanced tree. To find the minimum, we can simply create a variable that always
keeps track of the minimum when we are inserting.



== Sorting and Searching
[source,C]
----
void qsort(void *base, size_t nel, size_t width, int (*compare)(const void *,
                                                                const void *));

int intcompare(int *i, int *j) {
    if (*i > *j) return 1;
    if (*j > *i) return -1;
    return 0;
}

qsort(a, n, sizeof(int), intcompare);
----

=== Heap Sort
----
SelectionSort(A)
    for i = 1 to n do
        sort[i] = find-minimum from A
        Delete-minimum from A
    return sort
----
It takes O(1) time to remove the smallest item from an unsorted array once it
has been located, but O(n) time to find the smallest item. These are operations
supported by a priority queue. So let's replace our current data structure with
a better one! Let's use a heap or a balanced priority queue. Operations now take
O(log n)

heapsort is nothing but an implementation of selection sort using the right data
structure.

==== Heaps
Heaps are awesome for priority queue operations and extract-min. A heap-labeled
tree is defined such that the key labeling of each node dominates the key
labeling of its children. In a min-heap, a node dominates its children
containing a smaller key than they do, while in a max-heap, parent nodes
dominate by being bigger.

----
Min-heap

Data: 1492, 1783, 1776, 1804, 1865, 1945, 1963, 1918, 2001, 1941

              1492

     1783             1776

  1804       1865   1945  1963
1918 2001   1941
----

===== Constructing heaps
Heaps can be constructed incrementally, by inserting each new element into the
left-most open spot in an array, but this does not ensure the dominance ordering
of the keys.

Solution is to swap any such disattisfied element with its parent. We need to
bubble up after swapping so that everything is satisfied.

P.S: read more about arrays and binary trees.

Top element is the minimum value. Removing minimum leaves a hole in the array.
This can be 'solved' by moving the right-most leaf into the first position. Now
bubble down till the heap structure is satisfied again (heapify).


Read more about that!
There is also a faster insertion sort using same principle.

=== Mergesort

