== Book Notes from Algorithm Design Manual 2nd Edition
I'll be reading this book and hopefully posting notes and code snippets for each
category of algorithms

== Chapter 1: Introduction to Algorithm Design
[source,C]
----
// insertion sort
void insertion_sort(int *items, int n) {
    int i;
    int j;

    for (i = 1; i < n; ++i) {
        j = i;
        while ((j != 0) && (items[j] < items[j - 1])) {
            swap(&items[j], &items[j - 1]);
            --j;
        }
    }
}
----

=== Algorithm vs Heuristics
Algorithm always produces a correct result, whereas heuristics do a good job
without providing any guarantee.

=== Proof by induction
Often mathematical induction is the method of choice to prove that an algorithm
is valid.

You prove a formula for some basis case like 1 or 2, then assume it is true all
the way till `n - 1` before proving it was true for general _n_ using the
assumption.

Recursion: program tests whether the input was some basis case 1 or 2. If not,
solve the bigger case by breaking it into pieces and calling the subprogram
itself to solve these pieces.

Recursion is mathematical indcution.

Proving the correctness of insertion sort...

- Basis case consists of a single elemnt, and a single element array is
  completely sorted
- In general, we can assume that the first `n - 1` elements of array A are
  completely sorted after `n - 1` iterations of insertion sort
- To insert one last element x to A, we find where it goes (between biggest
  element less than or equal to x and the smallest element greater than x). Done
  by moving all the greater elements back by one position, creating room for x
  in the desired location.

==== Inductive Proof subtle errors
- Boundary errors:: greater care is needed to properly deal with the special
  cases of inserting the minimum or maximum elements
- Cavallier extension claims:: more common class of error. e.g scheduling
  problem: adding a new segment might now contain none of the segments of any
  particular optimal solution prior to insertion

=== Modeling the Problem
Modeling is formulating your application in terms of precisely described,
well-understood problems.

Trees:: hierarchical relationships between items
Graphs:: relationships between arbitrary pair of objects. e.g Networks

Polygons:: regions in some geometric spaces.


== Chapter 2: Algorithm Analysis
=== RAM Model of Computation
It is a hyptothetical computer called the Random Access Machine.

* Each simple operation (+, -, *, if, call) takes exactly one time step
* Loops are not considered simple oeprations.
* Each memory access takes exactly one time step

We measure runtime by counting up the number of steps an algorithm takes on a
given problem instance.

=== Big Oh Notation
It ignores the difference between multiplicative constants.

----
O(n) -> upper bound. (Big Oh)

f(n) = O(g(n))
Condition: f(n) <= c g(n)


Ω(n) -> lower bound

f(n) = Ω(g(n))
Condition: f(n) >= c g(n)

Θ(n) --> upper and lower bound

f(n) = Θ(g(n))
Condition:  c~1~ g(n) <= f(n) <= c~2~ g(n)
----

=== Types of functions
----
Constant:: f(n) = 1

Logarithmic:: f(n) = log n

Linear:: f(n) = n

Superlinear:: f(n) = n lg n

Quadratic:: f(n) = n^2^

Cubic:: f(n) = n^3^

Exponential:: f(n) = c^n^

Factorial:: f(n) = n!

n! > 2^n^ > n^3^ > n^2^ > n lg(n) > n > lg(n) > 1
----

=== Working with Big Oh

O(f(n)) + O(g(n)) -> O(max(f(n), g(n)))
Ω(f(n)) + Ω(g(n)) -> Ω(max(f(n), g(n)))
Θ(f(n)) + Θ(g(n)) -> Θ(max(f(n), g(n)))


So, `n^3^ + n^2^ + n + 1 = O(n^3)`

Intuition: At least half the bulk of `f(n) + g(n)` must come from the larger value
as `n -> ∞`. Thus, dropping the smaller function from consideration reduces the
value by at most a factor of half, which is just a multiplicative constant.

Multiplication is like repeated addition (e.g O(c f(n))) -> O(f(n)))

----
# e.g of this kind of multiplication: nested loops
O(f(n)) * O(g(n)) -> O(f(n) * g(n))
Ω(f(n)) * Ω(g(n)) -> Ω(f(n) * g(n))
Θ(f(n)) * Θ(g(n)) -> Θ(f(n) * g(n))
----

=== See discussion on insertion and selection sort in their source code
[source,C]
----
// selection sort
void selection_sort(int *s, int n) {
    int i;
    int j;
    int min;

    for (i = 0; i < n; ++i) {
        min = i;
        for (j = i + 1; j < n; ++j) {
            /* search for anything smaller than i */
            if (s[j] < s[min]) {
                min = j;
            }
        }
        /* swap the smallest item with i */
        swap(&s[i], &s[min]);
    }
}
/**
 * e.g
 * 64 25 12 22 11
 * 11 25 12 22 64
 * 11 12 25 22 64
 * 11 12 22 25 64
 * 11 12 22 25 64
 */
----
==== Analysis of selection sort
----
S(n) = E^n-1^~i=0~  E^n-1^~j=i+1~ 1  = E^n-1^~i=0~ n - i - 1
S(n) = (n - 1) + (n - 2) + (n - 3) + ... + 2 + 1
S(n) = n(n - 1) / 2
----

A basic rule of thumb in Big Oh analysis is that worst-case running time follows
from multiplying the largest number of times each nested loop can iterate.

e.g insertion sort => it's tricky to analyse the inner loop in the nested loop.
We could just assume that it runs `i` times.

This crude ``round it up'' analysis always does the job for Big Oh running time
bound.

=== String Pattern Matching
Substring pattern Matching

I: text string t and a pattern string p

O: Does t contain the pattern p as substring, and if so, where?

[source,C]
----
/* t == string, p == substring */
int findmatch(char *p, char *t) {
    int i;
    int j;
    int m, n;

    m = strlen(p); /* substring */
    n = strlen(t); /* string */

    for(i = 0; i <= (n-m); i++) {
        j = 0;
        while ((j < m) && (t[i + j] == p[j])) {
            j++;
        }
        if (j == m) return i;
    }
    /* failed */
    return -1;
}
----

`O((n-m)(m+2))`

m+2 => inner loops run at most m times, (+2 comes from j = 0 and if statement
inside outer loop)


`O((n-m)(m+2))` simplifies to O(nm - m^2^). Since O() means upper bound, we can get
rid of the -m^2^.

O(nm)


=== Matrix multiplication
----
for (i = 1; i <= x; ++i) {
    for(j = 1; j <= y; ++j) {
        c[i][j] = 0;
        for(k=1; k <= z; ++k) {
            c[i][j] += A[i][k] * B[k][j];
        }
    }
}
----
O(n^3^)

=== Logarithms and Their Applications
==== Binary Search
O(log n)

A binary tree of height 1 can have up to 2 leaf nodes, while a tree of height 2
can have up to 4 leaves.

To account for n leaves:

n = 2^h^

h = log~2~ n

==== Exponentiation
If we want to do a^n^, we could do so by:

a^n^ = (a^(n/2)^)^2^ <- even

a^n^ = a(a^(lower(n/2)^)^2^ <- odd

We have halved the size of our exponent at the cost of, at most, two
multiplications.

So O(lg n) multiplications needed

----
func power(a, n)
    if (n = 0) return 1
    x = power(a, lower(n/2))
    if (n is even) then return(x^2)
    else return (a * x ^ 2)
----

This illustrates an important principle of divide and conquer. It always pays to
divide a job as evenly as possible.

Logarithms arise whenever things are repeatedly halved or doubled.

== Chapter 3: Data Structures
Changing a data structure in a slow program can work the same way an organ
transplant does in a sick patient.

Data structures can be classified as either contiguous or linked, depending upon
whether they are based on arrays or pointers.

- Contiguous => array, heaps, hash tables

- Linked => composed of distinct chunks of memory bound together by pointers,
and include lists, trees, and graph adjacency lists

Advantages of contiguous

- Constant-time access given the index
- Space efficiency
- Memory locality

Downsize of arrays is that we cannot adjust their size in the middle of a
program's execution.


Pointers:
[source,C]
----
typedef struct list {
    item_type item;
    struct list *next;
} list;
----

Stack:: push: pop
Queue:: enqueue, deque
Dictionary:: search(k), insert(x), delete(x)

Additional features of dictionary:: max, min -> find item with largest key <--
used in priority queue.

View binary search as a binary tree.

[source,C]
----
void tranverse_tree(tree *t) {
    if (t != NULL) {
        traverse_tree(t->left);
        process_item(t->item);
        traverse_tree(t->right);
    }
}

void insert_tree(tree **t, item_type x, tree *parent) {
    tree *p;
    if (*t == NULL) {
        p = malloc(sizeof(tree));
        p->item = x;
        p->left = p->right = NULL;
        p->parent = parent;
        *t = p;
        return;
    }

    if (x < (*t)->item) insert_tree(&((*t)->left), x, *t);
    else insert_tree(&((*t)->right), x, *t);
}
----

Binary trees are good if the tree is perfectly balanced to implement dictionary
operations (searching, inserting, deleting)

If the user inserts keys in sorted order, this will produce a skinny linear
height tree where only right pointers are used. [1]

Binary trees can have heights ranging from lg n to n.

[1]: To fix [1], what would be better is an insertion/deletion procedure which
adjusts the tree a little after each insertion, keeping it close enough to be
balanced so the maximum height is logarithmic.

==== Priority Queues
e.g schedule jobs according to their importance relative to other jobs.
Scheduling requires sorting them by importance, and then evaluating them in this
sorted order.

Priority queues are data structures that provide more flexibility than simple
sorting, because they allow new elements to enter a system at arbitrary
intervals.

insert, find-minimum, find-maximum, delete-minimum, delete-maximum

Dating is the process of extracting the most desirable person from the data
structure, spending an evening to evaluate them better, and then reinserting
them into the priority queue with a possibly revised score.

We can implement a priority queue using an unsorted array, a sorted array, or a
balanced tree. To find the minimum, we can simply create a variable that always
keeps track of the minimum when we are inserting.


== Chapter 4: Sorting and Searching
[source,C]
----
/* e.g of C quicksort algorithm where order is determined by the intcompare
 * method */
void qsort(void *base, size_t nel, size_t width, int (*compare)(const void *,
                                                                const void *));

int intcompare(int *i, int *j) {
    if (*i > *j) return 1;
    if (*j > *i) return -1;
    return 0;
}

qsort(a, n, sizeof(int), intcompare);
----

=== Heapsort: Fast sorting via Data structures

----
/* basic pseudo code of selection sort */
SelectionSort(A)
    for i = 1 to n do
        sort[i] = find-minimum from A
        Delete-minimum from A
    return sort
----

It takes O(1) time to remove the smallest item from an unsorted array once it
has been located, but O(n) time to find the smallest item. These are operations
supported by a priority queue. So let's replace our current data structure with
a better one! Let's use a heap or a balanced priority queue. Operations now take
O(log n)

Heapsort is nothing but an implementation of selection sort using the right data
structure.

==== Heaps
Heaps are awesome for priority queue operations and extract-min. A heap-labeled
tree is defined such that the key labeling of each node dominates the key
labeling of its children. In a min-heap, a node dominates its children
containing a smaller key than they do, while in a max-heap, parent nodes
dominate by being bigger.

They work by maintaining a partial order on the set of elements which is weaker
than the sorted order, yet stronger than random order.

There is no implied ordering between siblings and cousins for an in-order
traversal.

Heaps are extremely efficient for priority queues.

The most natural implement would involve pointers. However we can store this
data as an array of keys, and use the position of the keys to implicitly satisfy
the role of the pointers.

We will store the root of the tree in the first position of the array, and its
left and right children in teh second and third positions, respectively. We will
store the 2^n^ keys on the _n_th level. We assume that the array starts with
index 1.

[source,C]
----
typedef struct {
    item_type q[PQ_SIZE + 1];   /* body of queue */
    int n;                      /* number of queue elements */
} priority_queue;
----

The left child of _k_ sits in position _2k_ and the right child in _2k + 1_,
while the parent of _k_ holds court in position `lower(n/2)`.

[source,C]
----
pq_parent(int n) {
    if (n == 1) return -1;
    else retrn ((int) n / 2);
}

pq_young_clild(int n) {
    return 2 * n;
}
----

This implicit representation of binary trees saves memory, but is less flexible
than using pointers. We cannot move subtrees around by just changing a single
pointer. This loss of flexibility means we cannot use this to represent binary
search trees, but it works just fine for heaps.

We cannot efficiently search for a particular key in a heap since binary search
does not work because a heap is not a binary search tree.

----
Min-heap

Data: 1492, 1783, 1776, 1804, 1865, 1945, 1963, 1918, 2001, 1941

              1492

     1783             1776

  1804       1865   1945  1963
1918 2001   1941
----

===== Constructing heaps
Heaps can be constructed incrementally, by inserting each new element into the
left-most open spot in an array, but this does not ensure the dominance ordering
of the keys.

Solution is to swap any such dissatisfied element with its parent. We need to
bubble up after swapping so that everything is satisfied.

[source,C]
----
pq_insert(priority_queue *q, item_type x) {
    if (q->n >= PQ_SIZE) {
        printf("Warning: prioriy queue overflow");
    } else {
        q->n = (q->n) + 1;
        q->q[q->n] = x;
        bubble_up(q, q->n);
    }
}

bubble_up(priority_queue *q, int p) {
    if(pq_parent(p) == -1) return;

    /* if parent bigger than node */
    if (q->q[pq_parent(p)] > q->q[p]) {
        pq_swap(q, p, pq_parent(p));
        bubble_up(q, pq_parent(p));
    }
}

pq_init(priority_queue *q) {
    q->n = 0;
}

make_heap(priority_queue *q, item_type s[], int n) {
    int i;
    pq_init(q);
    for(i = 0; i < n; ++i) {
        pq_insert(q, s[i]);
    }
}
----

Since the height of an n-element heap is lg n, each insertion takes at most
O(log n) time.

Top element is the minimum value. Removing minimum leaves a hole in the array.
This can be 'solved' by moving the right-most leaf into the first position. Now
bubble down till the heap structure is satisfied again (heapify).

[source,C]
----
bubble_down(priority_queue *q, int p) {
    int c;         /* child index */
    int i;         /* counter */
    int min_index; /* index of lightest child */

    c = pq_young_child(p);
    min_index = p;

    /* i = 0 and 1, representing child 1 and child 2 */
    for (i = 0; i <= 1; ++i) {
        if((c + i) <= q->n) {
            if (q->q[min_index] > q->q[c + i]) min_index = c + i;
        }
    }
    if (min_index != p) {
        pq_swap(q, p, min_index);
        bubble_down(q, min_index);
    }
}
----

Exchanging the maximum element with the last element and calling heapify
repeatedly gives an O(n log n) sorting algorithm, named Heapsort

[source,C]
----
heapsort(item_type s[], int n) {
    int i;              /* counters */
    priority_queue q;   /* heap for heapsort */

    make_heap(&q, s, n);

    for(i = 1; i < n; ++i) {
        s[i] = extract_min(&q);
    }
}
----
Remember how in selection sort we always try to find the minimum, then swapping.
Very similar concept.

==== Faster Heap Construction
We usually construct our heap by using `bubble_up`. Surprisingly, heaps can be
constructed even faster by using our `bubble_down` procedure and some clever
analysis.

[source,C]
----
make_heap(priority_queue *q, item_type s[], int n) {
    int i;          /* counter */

    q->n = n;
    for (i = 0; i < n; ++i) q->q[i + 1] = s[i];
    for (i = q->n; i >= 1; i--) bubble_down(q, i);
}
----
At first it seems it would take O(n log n)... analysis pg 128.

=== Sorting by incremental insertion
Select an arbitrary element from the unsorted set, and put it in the proper
position in the sorted set.

e.g insertion sort

Insertion sort is perhaps the simplest example of the incremental insertion
technique, where we build up a complicated structure on _n_ items by first
building it on n - 1 items and then making the necessary changes to add the last
item.

Use more efficient data strucutures help here. e.g balanced search tree.

=== Mergesort: Sorting by Divide-and-Conquer

----
MergeSort(A[1, n])
    Merge(MergeSort(A[1, lower(n/2)]), MergeSort(A[lower(n/2) + 1, n]))
----

The basis case of the recursion occurs when the subarray to be sorted consists
of a single element.

The efficiency of mergesort depends upon how efficiently we combine the two
sorted halves into a single sorted list.

Mergesort is a great algorithm for sorting linked lists, because it does not
rely on random access to elements as does heapsort or quicksort. Its primary
disadvantage is the need for an auxilliary buffer when sorting arrays.

[source,C]
----
merge(item_type s[], int low, int middle, int high) {
    int i;                  /* counter */
    queue buffer1, buffer2; /* buffers to hold elements for merging */

    init_queue(&buffer1);
    init_queue(&buffer2);

    for(i = low; i <= middle; i++) enqueue(&buffer1, s[i]);
    for(i = middle + 1; i <= high; i++) enqueue(&buffer2, s[i]);

    i = low;

    while(!(empty_queue(&buffer1)|| empty_queue(&buffer2))) {
        if(headq(&buffer1) <= headq(&buffer2))
            s[i++] = dequeue(&buffer1);
        else
            s[i++] = dequeue(&buffer2);
    }

    /* only one of those should actually run */
    while(!empty_queue(&buffer1)) s[i++] = dequeue(&buffer1);
    while(!empty_queue(&buffer2)) s[i++] = dequeue(&buffer2);
}
----

=== Quicksort: Sorting by Randomization
Suppose we select a random item _p_ from the _n_ items we seek to sort.
Quicksort separates the _n - 1_ other items into two piles: a low pile
containing all the elements that appear before _p_ in sorted order and a high
pile containing all the elements that appear after _p_ in sorted order.

Such paritioning buys us two things. First, the pivot element _p_ ends up in the
exact array position it will reside in the final sorted order. Second, after
paritioning, no element flops to the other side in the final sorted order. Thus
we can now sort the elements to the left and the right of the pivot
independently.

[source,C]
----
quicksort(item_type s[], int l, int h) {
    int p;

    if ((h - l) > 0) {
        p = partition(s, l, h);
        quicksort(s, l, p - 1);
        quicksort(s, p + 1, h);
    }
}
----

We can partition the array in one linear scan for a particular pivot element by
maintaining three sections of the array: less than the pivot (to the left of
firsthigh), greater than or equal to the pivot (between firsthigh and i), and
unexplored (to the right of i).

[source,C]
----
/* this is the in-place version */
int partition(item_type s[], int l, int h) {
    int i;          /* counter */
    int p;          /* pivot element index */
    int firsthigh;  /* divider position for pivot element */

    p = h;
    /* firsthigh tracks the item bigger than the pivot */
    firsthigh = l;

    /* s[p] is our pivot value */
    for (i = l; i < h; i++) {
        if (s[i] < s[p]) {
            swap(&s[i], &s[firsthigh]);
            /* only increment firsthigh if swapped */
            firsthigh++;
        }
    }
    /* move pivot to correct position */
    swap(&s[p], &s[firsthigh]);
    return(firsthigh);
}
----

If we consistently gets unlucky, the pivot always splits the array as
unequally as possible, i.e pivot is always largest or smallest number. We are
left with _n - 1_ subproblems. We'll end up with worse-case  Θ(n^2^)

==== Intuition: Expected case for Quicksort
Expected performance depends on the height of the partition tree. Whenever the
pivot is near the center of the sorted array, we get a good split and realize
the same performance as mergesort.

The best possible selection for the pivot would be the median key. But we only
have probability 1/n of selecting this key.

Suppose that a key is good enough pivot that it is ranked n/4 to 3n/4 in the
space of all keys sorted. Thus, on average we pick a good enough pivot with
probability 1/2.

On average, random quicksort partition trees are very good.

=== Randomized algorithms
Now suppose we add an initial step to our algorithm where we randomly permute
the order of the _n_ elements before we try to sort them. Such a permutation can
be constructed in O(n) time.

Randomization is a good tool to improve algorithms with bad worst-case but good
average-case complexity.

=== Is quicksort really quick?
The RAM model and Big Oh analysis provide too coarse a set of tools to make the
type of distinction. Experiments show that quicksort is two or three times
faster than mergesort or heapsort. The primary reason is that the operations in
the inner loop are simpler.

=== Distribution Sort: Sorting via Bucketing
We could sort names for a telephone book by partitioning them according to the
first letter of the last name. This will create 26 different piles, or buckets,
of names. We can then proceed to sort each pile individually and just
concatenate the bunch at the end.

If the names are distributed evenly among the buckets, the resulting 26 problems
should each be substantially smaller than the original problem. Further, by now
partitioning each pile based on the second letter, we generate smaller and
smaller piles.

Bucketing is effective when we know the distribution of data will be roughly
uniform.

=== Lower bounds for sorting
To sort n items, we have to look at the n items, so the lower bound will be
Ω(n).

Continued discussion pg 130.


=== Divide and Conquer
One of the most powerful techniques for solving problems is to break them down
into smaller, more easily solved pieces.

== Chapter 5: Graph Traversal
A graph G = (V, E) consists of a set of vertices V together with a set E of
vertex pairs or edges. Graphs are important because they can be used to
represent essentially any relationship.

The key to solving many algorithmic problems is to think of them in terms of graphs.

The key to using graph algorithms effectively in applications lies in correctly
modeling your problem so you can take advantage of existing algorithms.


=== Flavors of graphs
Properties of Graphs

- Undirected vs directed
  edge (x, y) implies (y, x) for undirected edges.

- Weighted vs Unweighted
  Each edge is assigned a numerical value, or weight. The edges of a road
  network might be weighted with their length, drive-time, or speed limit,
  depending upon the application.

  Difference between weighted and unweighted graphs is important when finding
  the shortest path.

- Simple vs non-simple
  Certain types of edges complicate the task of working with graphs. A
  _self-loop_ is an edge (x, x) involving one vertex. An edge (x, y) is a
  multiedge if it occurs more than once in the graph.

- Sparse vs Dense

- Cyclic vs Acyclic
  An acyclic graph does not contain any cycles. e.g Trees.

- Embedded vs topological
  A graph is embedded if the vertices and edges are assigned geometric positions.

- Implicit vs explicit
  Certain graphs are not explicitly constructed and then traversed, but built as
  we use them. A good example is the backtrack search.

- Labeled vs unlabeled
  Each vertex is assigned a unique name or identifier in a labeled graph to
  distinguish it from all other vertices.
